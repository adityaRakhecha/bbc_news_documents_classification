# -*- coding: utf-8 -*-
"""paramAI_assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13Jc22V3W4KJpidTagDUq6mSR0vOUfI3h

**Import necessary packages**
"""

import os

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

import nltk
from nltk.corpus import stopwordsHere is a dataset consist of 2225 documents from the BBC news website comprising stories in five topical areas for the annual year 2004-2005.

Each document corresponds to news and we have 5 classes - business, entertainment, politics, sport, tech. 

You can find the document related to a specific class in their respective folders. i.e, documents related to entertainment class can be found in the entertainment folder and likewise for the other classes.
from nltk.stem import WordNetLemmatizer

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.pipeline import Pipeline
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix

from gensim.models.word2vec import Word2Vec
from collections import Counter, defaultdict

import seaborn as sns

"""**Data Preparation**

1. Load the raw data zip file and unzip it
"""

# Upload the raw dataset on colab
!unzip bbc-fulltext.zip
# !ls "./bbc"

# Get all the sub-directories name
dir = './bbc/'
_, dirs, _ = os.walk(dir).__next__()
print(dirs)
class_names = dirs

# Print total number of files in each sub-directory
for file in dirs:
  print(file , " : ", len(os.listdir(dir + file)))

"""2. Dataset creation"""

file_name = []
text = []
class_name = []

for folder in dirs:
  
  for file in os.listdir(dir + folder):
    
      if file.endswith(".txt"):
        
          file_path = os.path.join(dir,folder,file)
          file_name.append(file)
          with open(file_path, 'rb') as f:
            text.append(f.read())
          class_name.append(folder)
          
print(len(file_name),len(text),len(class_name))

"""Let's see how our text data looks like:"""

text[2]

"""From above result we can see:
1. The text is in bytes form thus we need to decode it into string type,
2. There are unnecessary characters/tags like \n, "  " and \ . We have to remove them with " ".
3. There are unnecessary symbols like ?, ;, :, etc we have to remove them as they dont have any significance in our article classification objective.
4. Lots of stopping words like you, ourselves, ours, it, it's , etc we need to remove them too.

We will do all these steps to clean our data in Data cleaning section.

3. Now, let's create pandas DataFrame of our dataset and visulaize it.
"""

# del list
df = pd.DataFrame(list(zip(file_name, text, class_name)), columns = ['File_Name', 'Text', 'Class_Name'])

# Create a new column which contains length of each text file (article)
article_len = df['Text'].str.len()

# Insert the new column in our dataframe df
df.insert(2, 'Raw_Text_length', article_len)

df.head()

# Dimensions of our DataFrame
print(df.shape)

"""Now, let's calculate how the articles are distributed across each class (category) by plotting a Pie chart."""

# visualizing data
classes = {}
classes = df.Class_Name.value_counts()

counts = []
names = []
for name, count in classes.items():
  counts.append(count)
  names.append(name)
  
fig, ax = plt.subplots()
ax.pie(counts, labels=names, autopct='%1.1f%%')
ax.axis('equal')  # Equal aspect ratio ensures the pie chart is circular.
ax.set_title('Classes')

# plt.legend(loc='upper right')
plt.show()
#almost equal distribution of each class

"""From the above figure, we can see that the articles are almost equally distributed across each catogory thus our data is equally distributed.

**Data Cleaning**
"""

# decode the text into string type and clean the unnecessary tags
for i in range(len(text)):
  # I used ancient MS-DOS cp437 encoding
  text[i] = text[i].decode('cp437')
clean_string = lambda x: x.replace('\n', ' ').replace('  ', ' ')
text = [clean_string(item) for item in text]
df['Text'] = pd.DataFrame(text)

df['Text'].head()

# remove unnecessary symobols like 's, . , ; , ?, : , ! and ,
df['Text'] = df['Text'].str.replace("'s", "").replace(".", "").replace(";", "").replace("?", "").replace(":", "").replace("!", "").replace(",", "")

df['Text'].head()

"""It is important to lower case all the words in our text column because we don't want our classifier to see "My" and "my" differently as it would not make any sense."""

# convert all uppercase letter to lower case
df['Text'] = df['Text'].str.lower()

"""Removing stop words."""

# nltk.download('stopwords')

stop_words = list(stopwords.words('english'))

# print(stop_words[0:40])

print("There are total ", len(stop_words) ," stopwords.") # 179 stop words

# remove all the stopwords from our data
df['Updated_text'] = df['Text']

for s_w in stop_words:
  
  re_ex = r"\b" + s_w + r"\b" # add web url in reference
  df['Updated_text'] = df['Updated_text'].str.replace(re_ex, '')
  
# create a new column and calculate each article's text length  
df['Length_aftr_dc'] = df['Updated_text'].str.len()

df.head()

"""**Text Normalization** using Lemmatization :"""

# # download pre-trained Punkt tokenizer and WordNet database
nltk.download('punkt')
nltk.download('wordnet')

# create an object of WordNetLemmatizer class
wordnet_lemma = WordNetLemmatizer()

lemmatized_text = []
num_articles = df.shape[0]

for i in range(0, num_articles):
  
  new_text = []
  
  text = df.loc[i]['Updated_text']
  splitted_text = text.split(" ") #.split(list("., "))
  
  for word in splitted_text:
    new_text.append(wordnet_lemma.lemmatize(word, pos="v")) #pos="a"))
#   print(new_text[0:2])  
  temp_text = " ".join(new_text) #gfg
  
  lemmatized_text.append(temp_text)

df['Lemma_text'] = lemmatized_text
df['Lemma_text_length'] = df['Lemma_text'].str.len()

"""Now, let's see how our raw data of article texts have been changed after Data cleaning and Text Normalization process."""

df['Raw_Text_length'] = pd.to_numeric(df['Raw_Text_length'])
raw_txt_len = df.groupby('Class_Name')['Raw_Text_length'].mean()
print("Average article length of each class before data cleaning and lemmatization:\n")
print(raw_txt_len)

df['Length_aftr_dc'] = pd.to_numeric(df['Length_aftr_dc'])
dc_txt_len = df.groupby('Class_Name')['Length_aftr_dc'].mean()
print("\nAverage article length of each class after data cleaning:\n")
print(dc_txt_len)

df['Lemma_text_length'] = pd.to_numeric(df['Lemma_text_length'])
lemma_txt_len = df.groupby('Class_Name')['Lemma_text_length'].mean()
print("\nAverage article length of each class after lemmatization:\n")
print(lemma_txt_len)

# visualize the difference by plotting a graph
plt.figure(figsize=(10,6))
raw_txt_len.plot()
dc_txt_len.plot()
lemma_txt_len.plot()
plt.legend()
plt.show();

"""From the above graph we can see that how average length of each class' article has been changed after Data cleaning and Text Normalization process.

Create final DataFrame:
"""

my_df = df[["File_Name","Lemma_text","Lemma_text_length","Class_Name",]]
my_df.head()

"""**Feature Engineering**"""

my_df.head()

# class label encoding
label_enc = LabelEncoder()
class_id_values = label_enc.fit_transform(my_df['Class_Name']) 
my_df.insert(4, 'Class_id', class_id_values)

my_df.head()

my_df['Class_id'].value_counts()

"""Train-Test_Split"""

.
X_train, X_test, y_train, y_test = train_test_split(my_df['Lemma_text'],my_df['Class_id'],
                                                    test_size=0.25, random_state=8)

print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

"""**Text representation**"""

# TfidfVectorizer

vectorizer = TfidfVectorizer(encoding = 'utf-8',
                        ngram_range = (1,2),
                        stop_words = None,
                        lowercase = False,
                        max_df = 1.0,
                        min_df = 10,
                        max_features = 400,
                        norm='l2',
                        sublinear_tf=True
                       )

# train-test set transformation
# train
features_train = vectorizer.fit_transform(X_train).toarray()
labels_train = y_train
# test
features_test = vectorizer.transform(X_test).toarray()
labels_test = y_test

# train
features_train = vectorizer.fit_transform(X_train).toarray()
labels_train = y_train
# test
features_test = vectorizer.transform(X_test).toarray()
labels_test = y_test

print(features_train.shape)
print(features_test.shape)

"""**Building Classifier**

For building a perfect classifier for our article class classification task, I will use Random Forest Model. First we will find best values for parameters of Random Forest using Randomized Search Cross Validation.
"""

# select parameters to be tuned

# n_estimators (The number of trees in the forest)
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 5)]

# max_depth (The maximum depth of the tree)
max_depth = [int(x) for x in np.linspace(20, 100, num = 5)]
max_depth.append(None)

# min_samples_split (The minimum number of samples required to split an internal node)
min_samples_split = [2, 5, 10]

# min_samples_leaf (The minimum number of samples required to be at a leaf node)
min_samples_leaf = [1, 2, 4]

# bootstrap (Whether bootstrap samples are used when building trees. If False, the whole datset is used to build each tree)
bootstrap = [True, False]

# max_features (The number of features to consider when looking for the best split)
max_features = ['auto', 'sqrt']



# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

print(random_grid)

# create model
rf_model = RandomForestClassifier(n_jobs=-1,random_state=8)

# Randomized Search Cross Validation
randomSCV = RandomizedSearchCV(estimator=rf_model,
                              param_distributions=random_grid,
                              n_iter=50,
                             scoring='accuracy',
                              cv=3,
                              verbose=1,
                               n_jobs=-1,
                              random_state=8)

randomSCV.fit(features_train, labels_train)

print("The best hyperparameters from Random Search are:")
print(randomSCV.best_params_)
print("========================================================================================================")
print("\nThe mean accuracy of a model with these hyperparameters is:")
print(randomSCV.best_score_ * 100)

# save the best model
my_rfc = randomSCV.best_estimator_
my_rfc

# fit model to training data and predict labels on test data
my_rfc.fit(features_train, labels_train)
pred_labels = my_rfc.predict(features_test)
# pred_labels[0:5]

# training accuracy
print(accuracy_score(labels_train, my_rfc.predict(features_train)) * 100)

print("=====================================================================================")

# test accuracy
print("The mean or classification accuracy of RandomForest model on test data is:")
rfc_accuracy = accuracy_score(labels_test, pred_labels) * 100
print(rfc_accuracy)

"""For further visualization of evaluation of performance our model I will use Confusion Matrix."""

# generate confusion matrix
# add web url in reference -- 
conf_matrix = confusion_matrix(labels_test, pred_labels)
conf_matrix

"""For better visualization, I will build heatmap of Confusion matrix"""

# create dataframe of Confusion Matrix
df_cm = pd.DataFrame(conf_matrix, index=class_names, columns=class_names)

# heatmap
fig = plt.figure(figsize=(10,10))
fontsize = 11

try:
    heatmap = sns.heatmap(df_cm, annot=True, fmt="d")
except ValueError:
    raise ValueError("Confusion matrix values must be integers.")

heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)
heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)

plt.ylabel('Predicted label')
plt.xlabel('True label')
plt.show();

"""**Classification using Word Embedding Model GloVe:**"""

# split text and remove duplicates
split_func = lambda x: list(dict.fromkeys(x["Lemma_text"].split()))

X = my_df.apply(split_func, axis=1)
y = my_df["Class_id"]

# Getting the embedding model
!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip glove.6B.zip

# Load and prepare word embeddings from GloVe Model
import struct

glove_dict = {}
all_words = set(w for words in X for w in words)
with open("glove.6B.50d.txt", "rb") as infile:
  for line in infile:
    parts = line.split()
    word = parts[0].decode("utf-8")
    if(word in all_words):
      nums = np.array(parts[1:], dtype=np.float32)
      glove_dict[word] = nums

# Total words in our dataset (dictionary) - X
print(len(all_words))

"""Building features"""

# First by averaging word vectors of all words in a text file (row/article)

class MeanEmbeddingVectorizer(object):
  def __init__(self, word2vec):
    self.word2vec = word2vec
    if len(word2vec)>0:
      self.dim=len(word2vec[next(iter(word2vec))])
    else:
      self.dim=0
    
  def fit(self, X, y):
    return self
  
  def transform(self, X):
    return np.array([
        np.mean([self.word2vec[w] for w in words if w in self.word2vec]
               or [np.zeros(self.dim)], axis=0)
        for words in X
    ])

glove_mean = Pipeline([("glove vectorizer", MeanEmbeddingVectorizer(glove_dict)), 
                        ("extra trees", ExtraTreesClassifier(n_estimators=500))])

cvs_mean = cross_val_score(glove_mean, X, y, cv=4).mean() * 100 
print("The mean or classification accuracy of pretrained GloVe (mean) model is:")
print(cvs_mean)

# Now usinfg tfidf weighting method

class TfidfEmbeddingVectorizer(object):
  def __init__(self, word2vec):
    
    self.word2vec = word2vec
    self.word2weight = None
    if len(word2vec)>0:
      self.dim=len(word2vec[next(iter(word2vec))])
    else:
      self.dim=0
    
  def fit(self, X, y):
    
    tfidf = TfidfVectorizer(analyzer=lambda x:x)
    tfidf.fit(X)
    
    # if a word was never seen - it must be at least as infrequent as any of the
    # known words - so the default idf is the max of known idf's
    max_idf = max(tfidf.idf_)
    self.word2weight = defaultdict(
        lambda: max_idf, 
        [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])

    return self
  
  def transform(self, X):
    
    return np.array([
        np.mean([self.word2vec[w] * self.word2weight[w]
                for w in words if w in self.word2vec] or
               [np.zeros(self.dim)], axis=0)
        for words in X
    ])

glove_tfidf = Pipeline([("glove vectorizer", TfidfEmbeddingVectorizer(glove_dict)), 
                        ("extra trees", ExtraTreesClassifier(n_estimators=500))])

cvs_tfidf = cross_val_score(glove_tfidf, X, y, cv=5).mean() * 100
print("The mean or classification accuracy of pretrained GloVe (tfidf weight) model is:")
print(cvs_tfidf)

"""**RESULTS**"""

result = [("Random Forest",rfc_accuracy), ("glove_mean",cvs_mean), ("glove_tfidf",cvs_tfidf)]

plt.figure(figsize=(10,6))
sns.barplot(x = [name for name, _ in result], y = [score for _, score in result]);

print(result)

